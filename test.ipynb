{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53183538-d6fe-4d65-bb04-39f550333b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Transformer Encoder is not available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import whisper\n",
    "import numpy as np\n",
    "\n",
    "from gradio import processing_utils\n",
    "from modeling.BaseModel import BaseModel\n",
    "from modeling import build_model\n",
    "from utils.distributed import init_distributed\n",
    "from utils.arguments import load_opt_from_config_files\n",
    "from utils.constants import COCO_PANOPTIC_CLASSES\n",
    "\n",
    "from demo.seem.tasks import *\n",
    "\n",
    "conf_files = \"configs/seem/focall_unicl_lang_demo.yaml\"\n",
    "\n",
    "opt = load_opt_from_config_files([conf_files])\n",
    "opt = init_distributed(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1807b18d-8e9f-4485-8d4d-e90b4803b09f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40da5190>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 2f286127-2146-4f92-a859-3b9a9fe91c9c)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/vocab.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40da57c0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 21d3190b-e82f-49b1-917d-11edecc3bd05)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/vocab.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/merges.txt (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40da5c40>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 986f1cdc-a82a-42fd-ae38-ab37020167c7)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/merges.txt\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/added_tokens.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40dc2100>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 6e5904fe-e246-4748-ab96-e5046608c205)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/added_tokens.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/special_tokens_map.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40de78e0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 8516c68d-f37f-4e54-8d0f-900c3ec32219)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/special_tokens_map.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40da5c10>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: ed9603a6-f80e-4db3-9cfa-3b842faf4b1c)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer_config.json\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/tokenizer.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fab40da57c0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 64d58059-1f48-4850-9d8d-9940d297f18f)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/tokenizer.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'openai/clip-vit-base-patch32'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'openai/clip-vit-base-patch32' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     cur_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFocal-L\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# this\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m BaseModel(opt, \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_pth)\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msem_seg_head\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mlang_encoder\u001b[38;5;241m.\u001b[39mget_text_embeddings(COCO_PANOPTIC_CLASSES \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m], is_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/seem/modeling/architectures/build.py:10\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model(model_name):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnkown model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_entrypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/seem/modeling/architectures/seem_model_demo.py:923\u001b[0m, in \u001b[0;36mget_seem_model\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_seem_model\u001b[39m(cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneralizedSEEM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/seem/modeling/utils/config.py:66\u001b[0m, in \u001b[0;36mconfigurable.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass with @configurable must have a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _called_with_cfg(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 66\u001b[0m     explicit_args \u001b[38;5;241m=\u001b[39m \u001b[43m_get_args_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_config_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     init_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexplicit_args)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/seem/modeling/utils/config.py:137\u001b[0m, in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_arg_names:\n\u001b[1;32m    136\u001b[0m         extra_kwargs[name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[0;32m--> 137\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_config_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# forward the other arguments to __init__\u001b[39;00m\n\u001b[1;32m    139\u001b[0m ret\u001b[38;5;241m.\u001b[39mupdate(extra_kwargs)\n",
      "File \u001b[0;32m~/seem/modeling/architectures/seem_model_demo.py:117\u001b[0m, in \u001b[0;36mGeneralizedSEEM.from_config\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m    115\u001b[0m extra \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_switch\u001b[39m\u001b[38;5;124m'\u001b[39m: task_switch}\n\u001b[1;32m    116\u001b[0m backbone \u001b[38;5;241m=\u001b[39m build_backbone(cfg)\n\u001b[0;32m--> 117\u001b[0m lang_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_language_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m    118\u001b[0m sem_seg_head \u001b[38;5;241m=\u001b[39m build_xdecoder_head(cfg, backbone\u001b[38;5;241m.\u001b[39moutput_shape(), lang_encoder, extra\u001b[38;5;241m=\u001b[39mextra)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Training Settings.\u001b[39;00m\n",
      "File \u001b[0;32m~/seem/modeling/language/__init__.py:10\u001b[0m, in \u001b[0;36mbuild_language_encoder\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model(model_name):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnkown model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_entrypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/seem/modeling/language/vlpencoder.py:190\u001b[0m, in \u001b[0;36mget_language_model\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_language_model\u001b[39m(cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLanguageEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/seem/modeling/utils/config.py:66\u001b[0m, in \u001b[0;36mconfigurable.<locals>.wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass with @configurable must have a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _called_with_cfg(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 66\u001b[0m     explicit_args \u001b[38;5;241m=\u001b[39m \u001b[43m_get_args_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_config_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     init_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexplicit_args)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/seem/modeling/utils/config.py:137\u001b[0m, in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_arg_names:\n\u001b[1;32m    136\u001b[0m         extra_kwargs[name] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[0;32m--> 137\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_config_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# forward the other arguments to __init__\u001b[39;00m\n\u001b[1;32m    139\u001b[0m ret\u001b[38;5;241m.\u001b[39mupdate(extra_kwargs)\n",
      "File \u001b[0;32m~/seem/modeling/language/vlpencoder.py:49\u001b[0m, in \u001b[0;36mLanguageEncoder.from_config\u001b[0;34m(cls, cfg)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, cfg):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# build up text encoder for seg\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMODEL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTEXT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     tokenizer_type \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKENIZER\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     51\u001b[0m     lang_encoder \u001b[38;5;241m=\u001b[39m build_lang_encoder(cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODEL\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer, cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVERBOSE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/seem/modeling/language/LangEncoder/__init__.py:23\u001b[0m, in \u001b[0;36mbuild_tokenizer\u001b[0;34m(config_encoder)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_encoder[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKENIZER\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     20\u001b[0m     pretrained_tokenizer \u001b[38;5;241m=\u001b[39m config_encoder\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRETRAINED_TOKENIZER\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_token\u001b[39m\u001b[38;5;124m'\u001b[39m: tokenizer\u001b[38;5;241m.\u001b[39meos_token})\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config_encoder[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOKENIZER\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip-fast\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2023\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2024\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2026\u001b[0m     )\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2029\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2030\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2031\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2032\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2033\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2034\u001b[0m     )\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2037\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'openai/clip-vit-base-patch32'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'openai/clip-vit-base-patch32' is the correct path to a directory containing all relevant files for a CLIPTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "cur_model = 'None'\n",
    "pretrained_pth = '~/autodl-tmp'\n",
    "\n",
    "if 'focalt' in conf_files:\n",
    "    pretrained_pth = os.path.join(pretrained_pth, \"seem_focalt_v0.pt\")\n",
    "    cur_model = 'Focal-T'\n",
    "elif 'focall' in conf_files:\n",
    "    pretrained_pth = os.path.join(pretrained_pth, \"seem_focall_v0.pt\")\n",
    "    cur_model = 'Focal-L'\n",
    "    # this\n",
    "\n",
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(COCO_PANOPTIC_CLASSES + [\"background\"], is_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d6823-ddd6-45b0-abee-a6f4c36a4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisper.load_model(\"base\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(image, task, *args, **kwargs):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        if 'Video' in task:\n",
    "            return interactive_infer_video(model, audio, image, task, *args, **kwargs)\n",
    "        else:\n",
    "            return interactive_infer_image(model, audio, image, task, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24728521-e600-470b-93a1-520d8a4125e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "input_image = Image.open('../2.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f99e93-fb31-44ff-a4a5-d19181b6052b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = interactive_infer_image(model, audio, input_image, ['Task'])\n",
    "print(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
